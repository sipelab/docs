---
title: "Scientific Data Analysis Pipelines and Reproducibility"
source: "https://medium.com/data-science/scientific-data-analysis-pipelines-and-reproducibility-75ff9df5b4c5"
author:
  - "[[Altuna Akalin]]"
published: 2021-07-05
created: 2025-04-22
description: "Pipelines are computational tools of convenience. Data analysis usually requires data acquisition, quality check, clean up, exploratory analysis and hypothesis driven analysis. Pipelines can automate…"
tags:
  - "clippings"
---
Photo by [Campaign Creators](https://unsplash.com/@campaign_creators?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## What do pipelines do? Why do we need them?

Pipelines are computational tools of convenience. Data analysis usually requires data acquisition, quality check, clean up, exploratory analysis and hypothesis driven analysis. Pipelines can automate these steps. They process raw data to a suitable format and analyze it with statistical tools or machine learning models in a streamlined way. In practical terms, a data analysis pipeline executes a chain of command-line tools and custom scripts. This usually provides processed data sets and a human readable report covering topics such as data quality, exploratory analysis etc.

In our field, raw data comes as text files containing sequencing reads. The reads have a 4-letter code (ACGT) and they originate from specific locations of the genome. We need to quality check the reads, align them to the genome, quantify them and run statistical/machine-learning models on them. Different command-line tools and custom scripts have to be run in sequence to achieve these tasks. If there is a problem in quality check or alignments, parts or all the steps need to be re-run with different parameters depending on the nature of the problem observed with the data. We may have to run this for hundreds of times, so automating at least part of these tasks via pipelines is beneficial.

## What is reproducibility? Why is it important?

Pipelines can be a great help when you have to process a dataset repeatedly with some changes in parameters or when you process multiple datasets. Since basic data processing and analysis tasks can take a lot of hands-on time, automating certain parts of these saves time. Researchers can then spend more time on visualization, communication of results or tailor made statistical/machine-learning analysis. Because of this convenience many researchers are creating pipelines and sharing them with the community via [publications](https://www.ncbi.nlm.nih.gov/pubmed/?term=data+analysis+pipeline). Normally, when you share the pipeline you would like to make sure that your pipeline will produce the same output for other users when provided with the same input data. How can one install the exact same pipeline with the exact dependencies its creator using and make sure it produces the same output? Although it sounds like a trivial question, reports regarding “[reproducibility crisis in science](https://en.wikipedia.org/wiki/Replication_crisis)” shows that it is not very easy to achieve this. Other researchers repeatedly fail to reproduce published experiments. This “reproducibility crisis” is not limited to fields such as biology or psychology. Computational fields [also suffer from this](http://science.sciencemag.org/content/334/6060/1226.full).

There are a couple of criteria for reproducible data analysis.

***Data and metadata availability:*** Data and metadata should be available without question. Without these, there is a no way you can reproduce an analysis. In our research domain, data and metadata usually deposited to public databases after publication.

***Transparency:*** There should be complete transparency of the code you are using and the dependencies you need to run the code. This also extends to source code availability of your dependencies. It is undesirable to have a tool whose behaviour crucially depends on a proprietary binary blob / black box. In addition, you need to know exact versions and configurations of the dependencies to have a shot at reproducing the data analysis pipeline. Preferably, the installation procedure keeps track of the different dependency structures and installs everything you need, see the point below.

***Ease of installation (installability):*** Computational analysis tools and pipelines should make the effort to be easily installable. I think many of us will be deterred if the pipeline has many dependencies that have to be installed separately. This will remain so even if we are promised to get a working pipeline that reproduces the authors version after we go through installation of each dependency diligently. The more dependencies a pipeline has, the more it is likely that at least one of them will be a problem during installation. Many published scientific software can not be installed. Studies claim at least 50% of the published software is uninstallable \[see [here](http://cs.brown.edu/~sk/Memos/Examining-Reproducibility/) & [here](http://reproducibility.cs.arizona.edu/tr.pdf)\]. I suspect that for the pipelines, many with more complicated dependencies, the situation is worse. People who have gone through poorly written readme files and tried to install all the dependencies know very well why “ease of installation” is important.

***Runtime environment reproducibility:*** The installed software should behave the same in every machine, in the sense that we need to install the very same software in every machine. Achieving this is not straightforward because the software depends on many different things from compilers, to system libraries to 3rd-party software and libraries needed. You need to control this complex system of dependencies if you want to build software exactly the same way on different machines and get the same software. The version of dependencies and how they are built will have an effect on the software you are trying to install. For example if the software you are trying to install requires Boost C++ library, having Boost version 1.68 might produce differences compared to having version 1.38. There might be bug fixes or improvements that could change the behaviour of the software we are trying to install. Therefore, this software can behave differently even though same version is installed on two different machines because of dependency differences.

If you can install the same exact software, built the same way with exactly the same dependencies down to the compiler, you have good chances at reproducing the runtime environment across different machines and therefore the analysis with the same input data. Only exception here is that if the software has some stochastic component that you can not control then it will not be possible to reproduce the analysis. For example, k-means clustering algorithm might produce different clustering results every time depending a random initialization procedure. If we are not able to control that behavior by setting random seed we won’t be able to reproduce the results.

Essential ingredients of data analysis reproducibility. Reproducibility requires availability of data and being able to use the same exact software.

## Reproducibility spectrum for computational analysis

The main aim of the scientists is to analyze data as quickly and accurately as possible, so any effort to make things nice for the future version of ourselves or other potential users seems like a misplaced effort. For many scientific journals, putting your code on github will suffice in terms of reproducibility, and that is recent requirement only from some journals. We and many others argue this is not always enough.

In practical terms, what many of us have in mind when we want to use a published data analysis tool is the following: 1) “can I install the published software at all and get similar results? 2) ”can I use this in my research as well?”. At this point, we just want to use the software, and will not try to reproduce the analysis in the publication unless it is part of the test case or examples. So, many researchers’ understanding of reproducibility is tied to installability.

However, if we are really concerned about the reproducibility, the correct question to ask is “Provided that I can install it, can I get identical results to the published paper with same input data?”. Even more general but related question would be “Can I get the same results with the same input data when I install the software on different systems?”. I think answering these questions positively requires strict control of the dependencies of the data analysis pipeline. People who made the pipeline available should either copy their software environment and offer it as a virtual machine or as a container. Or they should make sure when users install the pipeline they get identical versions of the dependencies. Each dependency should be built in an identical manner and the installed software should be bit-by-bit identical on your machine and on the users machine. Not controlling dependencies strictly will result in software that could behave differently. These differences in behaviour may change the results of the analysis.

All these different viewpoints or ignorance on reproducibility creates a reproducibility spectrum ranging from “not reproducible” to “gold standard”. While many see making the source code available online is enough, others require that more of the criteria mentioned above to be fulfilled.

Reproducibility spectrum observed in publications. Sharing data and code is seen as enough to reproduce the data analysis by many. However, this is not enough.

## Containers to the rescue

One of the most popular ways to deal with installability and reproduction of the runtime environment is using containers, which are light-weight virtual machines. Most popular containers in our domain are [docker](https://www.docker.com/) and lately [singularity](https://singularity.lbl.gov/) containers. Using this approach, complicated dependencies of a pipeline and the pipeline itself can be “containerized”. This means that if you can manage to install your pipeline and dependencies in this light-weight virtual machine, you can ship the container and be sure that anyone who uses that container will be able to reproduce your software environment. The results should be identical across different operating systems. Another approach is to provide dependencies of the pipelines as containers. In this setting, each tool you are using in your pipeline will be a container. There are some pipeline frameworks that can make use of this but I find this approach somewhat impractical.

The ease of installation and runtime reproduction come with a cost of reduced transparency and security. It is hard to verify exactly what are the contents of a container. Docker recommends use of docker files but they do not necessarily have the version of the software that is in the container. It is mostly a collection of commands installing software from package managers. Containers are also harder to maintain if you do not analyze data and develop code on dockerized environments exclusively. If you don’t, after creating your pipeline you must containerize it and check if you have identical results, which is additional work. There are efforts to make containers more secure, transparent and easy to work with but they are not widely adopted by the community.

## Package managers and reproducibility

Another way of providing installability and dependency management is using package managers. You can either build your pipeline as a bona fide software and package that using the package managers or provide list of packages from the package manager to be installed.

Regardless of the path you choose, a very popular approach is to use [Conda](https://conda.io/docs/) package manager. It is very easy to use, doesn’t require root access and you get pre-compiled binaries which greatly reduces installation times and works (most of the time) on windows, mac OS and linux. It is also relatively easier to package software for Conda. In addition, Conda has a large number of contributors and maintainers. These features aim to maximize the installability. However, Conda packages are not reproducible at all. You can get different binaries for the same name+version query at different times and there is no way to track which source files of dependencies produced that binary. The system environment where the software is built is not isolated. During the build, processes have access to other libraries that are not in the package recipe and also conda assumes certain low-level packages to be available in all environments. Their existence or the versions of them can affect the build and create different software on different systems. Bioconda is trying to circumvent some of these issues by building in containers.

While package managers generally provide installability, few are concerned with build reproducibility. If you can make sure the installed software is identical bit-by-bit on different systems, you can have runtime reproducibility and have the same results with same input data (minus the caveats mentioned in “*Runtime environment reproducibility*” subsection). [GNU Guix package manager](https://www.gnu.org/software/guix/manual/en/html_node/Package-Management.html) achieves that by strictly managing and keeping track of the full dependency graph of the software to be built. It makes way fewer assumptions about the system environment the software is built on and installed on. As it knows all the dependencies, it can build them in an isolated environment. The result of this process is usually bit-by-bit identical files after installation on different machines (see [here](https://academic.oup.com/gigascience/advance-article/doi/10.1093/gigascience/giy123/5114263) for assessment of bit-by-bit identity of various bioinformatics software built on different machines). It also provides containerization out-of-the-box. You can make docker and singularity containers for your pipelines and all their dependencies. The containers built by Guix contain exactly the dependencies that are needed by the pipeline — nothing more or less. For the reproducibility and robustness it provides, we opted to use Guix to manage dependencies of [our pipelines](http://bioinformatics.mdc-berlin.de/pigx).

However, Guix itself is only available for linux and requires a root-dameon for build isolation. Packaging software is not as easy as conda, although R and Python importers do almost all the work for packages from those frameworks. For other software, it might be more complicated. In addition, users might find it difficult to manage the flexibility of Guix. You can have different versions of the same software and you can roll back to the older versions but [managing that](https://www.gnu.org/software/guix/blog/2018/multi-dimensional-transactions-and-rollbacks-oh-my/) can be confusing.

## The effect of pipeline framework on reproducibility

The pipeline frameworks have little to no effect on reproducibility. Main challenge for reproducibility is providing installability and reproduction of run time environment. Pipeline frameworks provide a structured way to pipe tools/scripts together and provide additional features such as easy parallelization. Pipeline frameworks such as snakeMake, Rufus and nextflow have different advantages and disadvantages reviewed [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5429012/).

To reiterate, choice of framework is not the major problem for reproducing analysis pipelines. For [our own purposes](http://bioinformatics.mdc-berlin.de/pigx) we went with snakeMake due to wide usage in bioinformatics community, but we can easily switch to something else and still be reproducible.

## Parting thoughts

Making the data analysis pipelines reproducible should be the aim for the scientists if they want their tools or approach to be used by a wider audience. The quick and dirty way to do that is to use containers. However, that will not be easy on the maintenance or future upgrades. Needles to say, it is not a transparent way to deploy analysis pipelines, and research needs maximum transparency. A more sensible way to do this is to package the pipeline in a package manager. Conda package manager is easy to use but can never be fully reproducible. But its ease of use and large community support in bioinformatics makes it a go-to-tool for many. More reproducibility can be achieved via Guix package manager with additional perks few other package managers can offer. It provides software build reproducibility by design and source-to-binary transparency. Like few other package managers, Guix comes with project/user specific software profiles where you can have different versions of the same software without any conflict. You can also build docker and singularity containers effortlessly. Sharing a Guix manifest file along with your script will be enough to reproduce the software side of your analysis.

Practically, all the options (Conda, containers and Guix) are way better than just sharing the code and the documentation. But in my opinion Guix is closer to the gold standard of reproducibility which includes transparency, ease of installation and runtime environment reproducibility.

Comparison of different ways of deploying pipelines with respect to criteria for reproducibility. Containers and Guix have medium installability because they require a framework to be installed first before users can use it, this first step most often needs root access. Conda does not keep track of full dependency graphs and makes assumptions about your system that’s why it doesn’t have high transparency or reproducible runtime environment. Containers have low transparency because it is hard to tell what is exactly in them even with dockerfiles present.